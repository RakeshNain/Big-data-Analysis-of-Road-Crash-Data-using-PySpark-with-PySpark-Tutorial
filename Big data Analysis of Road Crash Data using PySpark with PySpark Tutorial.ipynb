{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data Analysis of Road Crash Data using PySpark with PySpark Tutorial.\n",
    "\n",
    "Contributor: _Rakesh Nain_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this article, I will be using data about road crashes in South Australia. Data is given by The Department of Planning, Transport and Infrastructure ( DPTI ), South Australia. I will be using PySpark and try to do small Data Analysis using parallel computing with a brief overview of PySpark concepts.\n",
    "I will be demonstrating parallel computing using three types of data structures: RDDs, PySpark DataFrame, SparkSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information on Dataset:\n",
    "The data used here is the Road Crash Data from 2012–2019 for South Australia prepared by the Department of Planning, Transport and Infrastructure (DPTI). The data is available on the website https://data.sa.gov.au. The datasets contain various details about the crash events including the vehicle and the people involved in the crash. In this article, only two datasets i.e. Crash and Units are considered. For more detailed information on the dataset, please refer to the Metadata file in the given [website](https://data.sa.gov.au). This [website](https://data.sa.gov.au) also contains required road crash data set or you can download Metadata and Data from my [GitHub](https://github.com/RakeshNain/Big-data-Analysis-of-Road-Crash-Data-using-PySpark-with-PySpark-Tutorial.git).\n",
    "\n",
    "Note: In the dataset, the exact day of the crash is not released by the data provider, being considered as sensitive information. When displaying dates, please use the format ( Year-Month-Dayofweek ) E.g. (2017-January-Sunday)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is PySpark?\n",
    "PySpark is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Basically, PySpark is a module in python, you have to import it and use it, it will make your distributed/parallel computing highly easy to implement and fault tolerant.\n",
    "\n",
    "Let's import the required libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing parallel computing\n",
    "Before we go ahead, the reader should be aware that here we are doing parallel computing, not distributed computing. There is a difference between Parallel computing and Distributed computing. In Parallel Computing, computing is done parallelly among different cores of single machine whereas, in Distributed computing, computing is done parallelly on different machines. Concept of implementing parallel computing and distributed computing by PySpark are almost similar so I if you know how to do parallel computing using PySpark then you can easily do distributed computing using a similar concept. Since many readers will have only one machine so for this article I will be showing parallel computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring PySpark\n",
    "To run a PySpark application on the local(parallel computing)/cluster(distributed computing), a few configurations and parameters need to be set, for this, SparkConf provides configurations to run a Spark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local[*]: run Spark in local-mode(parallel computing) with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Big data Analysis of Road Crash Data\"\n",
    "\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SparkSession\n",
    "To create a SparkSession we build a SparkConf object that contains information about my application. Here I am running Spark locally with as many working processors as logical cores on my machine. Now let's create a SparkContext object using SparkSession, which tells Spark how to access a cluster of local cores in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a SparkContext object \n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis using RDDs\n",
    "In this section, I will be creating RDDs from the given datasets, performing partitioning in these RDDs and use various RDD operations to make queries for crash analysis.\n",
    "\n",
    "But 1st let's understand what is RDD, Resilient Distributed Dataset (RDD) is Spark's core abstraction for working with data. It is simply an immutable distributed collection of objects. Spark automatically distributes the data contained in RDDs across the cluster and parallelizes the operations performed on them. Each RDD is split into multiple partitions, which may be computed on different nodes/cores of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user defined classes. In Spark, all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. To understand these RDDs operation, you can download my juyter notebook(PySpark RDDs tutorial) from my [GitHub repository](https://github.com/RakeshNain/Big-data-Analysis-of-Road-Crash-Data-using-PySpark-with-PySpark-Tutorial.git) and data files required for \"PySpark RDDs tutorial\" are in the folder \"data for given tutorials\".\n",
    "\n",
    "The class `pyspark.SparkContext` creates a client which connects to a Spark cluster. This client can be used to create an RDD object. There are two methods from this class for directly creating RDD objects:\n",
    "* `parallelize()`\n",
    "* `textFile()`\n",
    " - `parallelize()` distribute a local **python collection** to form an RDD. Common built-in python collections include `list`, `tuple` or `set`.\n",
    " \n",
    " Examples: loading data using `parallelize()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default partitions:  8\n",
      "Manual partitions:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FROM A LIST\n",
    "# By default the number of partitions will be the number of threads\n",
    "data_list = [i for i in range(10)]\n",
    "#print(data_list)\n",
    "rdd = sc.parallelize(data_list)\n",
    "# You can verify the number of partitions of the data \n",
    "print('Default partitions: ',rdd.getNumPartitions())\n",
    "# the function parallelize can have a second argument to indicate manually how many\n",
    "# partitions for the data\n",
    "rdd = sc.parallelize(data_list,5)\n",
    "# Verify the new number of partitions of the data \n",
    "print('Manual partitions: ',rdd.getNumPartitions())\n",
    "# Show the data by performing the action *collect*\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— `textFile()` function reads a text file and returns it as an **RDD of strings**. Usually, you will need to apply some **map** functions to transform each element of the RDD to some data structure/type that is suitable for data analysis. When using `textFile()`, each line of the text file becomes an element in the resulting RDD. Examples: loading data from an external dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the \"Units\" csv files from downloaded 2012–2019 South Australia Road Crash Data into a single RDD.\n",
    "# creating RDD object by reading csv file\n",
    "units_rdd = sc.textFile('./units_data/*.csv')\n",
    "\n",
    "# Importing all the \"Crashes\" csv files from 2012–2019 into a single RDD.\n",
    "# creating RDD object by reading csv file\n",
    "crash_rdd = sc.textFile('./crash_data/*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each Unit and Crashes RDDs, removing the header rows and displaying the total count and first 10 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of units_rdd:  272591\n",
      "['\"2012-1-21/08/2019\",\"01\",0,,\"Pedal Cycle\",,\"South West\",\"Male\",\"035\",,,,,\"Straight Ahead\",\"001\",\"5074\",,', '\"2012-1-21/08/2019\",\"02\",0,\"SA\",\"Motor Cars - Sedan\",\"2009\",\"North West\",\"Female\",\"050\",\"SA\",\"C \",\"Full\",\"Unknown\",\"Leaving Private Driveway\",\"001\",\"5089\",,', '\"2012-2-21/08/2019\",\"01\",0,\"SA\",\"Motor Cars - Sedan\",\"1999\",\"West\",\"Male\",\"027\",\"SA\",,\"Provisional 1 \",\"Not Towing\",\"Straight Ahead\",\"002\",\"5008\",,', '\"2012-2-21/08/2019\",\"02\",0,\"SA\",\"Motor Cars - Sedan\",\"1993\",\"West\",,,,,,\"Not Towing\",\"Parked\",\"000\",,,', '\"2012-3-21/08/2019\",\"01\",0,\"SA\",\"Motor Cars - Sedan\",\"XXXX\",\"North East\",\"Male\",\"050\",\"SA\",,\"Full\",\"Trailer\",\"Straight Ahead\",\"006\",\"5107\",,', '\"2012-3-21/08/2019\",\"02\",0,\"SA\",\"Motor Cars - Sedan\",\"2006\",\"North East\",\"Male\",\"044\",\"SA\",,\"Full\",\"Not Towing\",\"Straight Ahead\",\"003\",\"5092\",,', '\"2012-3-21/08/2019\",\"03\",0,,\"Other Inanimate Object\",,,,,,,,,,,,,', '\"2012-4-21/08/2019\",\"01\",0,\"SA\",\"Station Wagon\",\"2009\",\"South West\",\"Female\",\"037\",\"SA\",,\"Full\",\"Not Towing\",\"Stopped on Carriageway\",\"001\",\"5159\",,', '\"2012-4-21/08/2019\",\"02\",0,\"VIC\",\"Station Wagon\",\"XXXX\",\"South West\",\"Male\",\"071\",\"QLD\",,\"Unknown\",\"Not Towing\",\"Straight Ahead\",\"002\",\"4066\",,', '\"2012-5-21/08/2019\",\"01\",0,\"SA\",\"Motor Cars - Sedan\",\"1998\",\"South East\",\"Male\",\"026\",\"UNKNOWN\",,\"Unknown\",\"Not Towing\",\"Straight Ahead\",\"001\",\"2763\",,']\n"
     ]
    }
   ],
   "source": [
    "# Removing the header of units_rdd\n",
    "units_header = units_rdd.first()\n",
    "units_rdd = units_rdd.filter(lambda row: row != units_header)   #filter out header\n",
    "print(\"Total count of units_rdd: \", units_rdd.count())\n",
    "print(units_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of crash_rdd:  127672\n",
      "['\"2012-1-21/08/2019\",\"2 Metropolitan\",\"STEPNEY\",\"5069\",\"CC OF NORWOOD,PAYNEHAM & ST PETERS\",2,0,0,0,0,2012,\"January\",\"Sunday\",\"04:30 pm\",\"060\",\"Not Divided\",\"Straight road\",\"Level\",\"Driveway or Entrance\",\"Sealed\",\"Dry\",\"Not Raining\",\"Daylight\",\"Right Angle\",\"02\",\"Driver Rider\",\"1: PDO\",\"No Control\",\"\",\"\",1330659.71,1671795.87,\"13306601671796\"', '\"2012-2-21/08/2019\",\"2 Metropolitan\",\"PARKSIDE\",\"5063\",\"CITY OF UNLEY\",2,0,0,0,0,2012,\"January\",\"Sunday\",\"09:10 am\",\"040\",\"Not Divided\",\"Straight road\",\"Level\",\"Not Applicable\",\"Sealed\",\"Dry\",\"Not Raining\",\"Daylight\",\"Hit Parked Vehicle\",\"01\",\"Driver Rider\",\"1: PDO\",\"No Control\",\"\",\"\",1329400.16,1668462.66,\"13294001668463\"', '\"2012-3-21/08/2019\",\"2 Metropolitan\",\"SELLICKS BEACH\",\"5174\",\"CITY OF ONKAPARINGA\",3,0,0,0,0,2012,\"January\",\"Wednesday\",\"11:30 am\",\"100\",\"Not Divided\",\"Straight road\",\"Slope\",\"Not Applicable\",\"Sealed\",\"Dry\",\"Not Raining\",\"Daylight\",\"Other\",\"01\",\"Driver Rider\",\"1: PDO\",\"No Control\",\"\",\"\",1313748.22,1624241.28,\"13137481624241\"', '\"2012-4-21/08/2019\",\"2 Metropolitan\",\"HINDMARSH\",\"5007\",\"CITY OF CHARLES STURT\",2,0,0,0,0,2012,\"January\",\"Wednesday\",\"10:20 am\",\"060\",\"T-Junction\",\"Straight road\",\"Level\",\"Not Applicable\",\"Sealed\",\"Dry\",\"Not Raining\",\"Daylight\",\"Rear End\",\"02\",\"Driver Rider\",\"1: PDO\",\"Stop Sign\",\"\",\"\",1325326.77,1672425.55,\"13253271672426\"', '\"2012-5-21/08/2019\",\"2 Metropolitan\",\"HINDMARSH\",\"5007\",\"CITY OF CHARLES STURT\",2,0,0,0,0,2012,\"January\",\"Wednesday\",\"03:30 pm\",\"060\",\"T-Junction\",\"Straight road\",\"Level\",\"Not Applicable\",\"Sealed\",\"Dry\",\"Not Raining\",\"Daylight\",\"Rear End\",\"01\",\"Driver Rider\",\"1: PDO\",\"Traffic Signals\",\"\",\"\",1326056.45,1673028.42,\"13260561673028\"', '\"2012-6-21/08/2019\",\"2 Metropolitan\",\"MELROSE PARK\",\"5039\",\"CC MITCHAM.                   \",4,0,0,0,0,2012,\"January\",\"Friday\",\"11:15 am\",\"060\",\"Cross Road\",\"Straight road\",\"Level\",\"Not Applicable\",\"Sealed\",\"Dry\",\"Not Raining\",\"Daylight\",\"Rear End\",\"02\",\"Driver Rider\",\"1: PDO\",\"Traffic Signals\",\"\",\"\",1325912.16,1664946.07,\"13259121664946\"', '\"2012-7-21/08/2019\",\"1 City\",\"NORTH ADELAIDE\",\"5006\",\"CITY OF ADELAIDE\",2,0,0,0,0,2012,\"January\",\"Thursday\",\"01:45 pm\",\"060\",\"One Way\",\"Straight road\",\"Level\",\"Not Applicable\",\"Sealed\",\"Dry\",\"Not Raining\",\"Daylight\",\"Side Swipe\",\"01\",\"Driver Rider\",\"1: PDO\",\"No Control\",\"\",\"\",1329438.85,1673172.36,\"13294391673172\"', '\"2012-8-21/08/2019\",\"2 Metropolitan\",\"HOLDEN HILL\",\"5088\",\"CITY OF TEA TREE GULLY\",2,0,0,0,0,2012,\"January\",\"Friday\",\"04:30 pm\",\"060\",\"T-Junction\",\"Straight road\",\"Level\",\"Not Applicable\",\"Sealed\",\"Dry\",\"Not Raining\",\"Daylight\",\"Rear End\",\"01\",\"Driver Rider\",\"1: PDO\",\"Traffic Signals\",\"\",\"\",1334713.27,1678543.84,\"13347131678544\"', '\"2012-9-21/08/2019\",\"2 Metropolitan\",\"REGENCY PARK\",\"5010\",\"CITY OF PORT ADELAIDE ENFIELD\",2,0,0,0,0,2012,\"January\",\"Saturday\",\"10:15 am\",\"060\",\"T-Junction\",\"Straight road\",\"Level\",\"Not Applicable\",\"Sealed\",\"Dry\",\"Not Raining\",\"Daylight\",\"Rear End\",\"02\",\"Driver Rider\",\"1: PDO\",\"Traffic Signals\",\"\",\"\",1326449.52,1676375.13,\"13264501676375\"', '\"2012-10-21/08/2019\",\"2 Metropolitan\",\"PROSPECT\",\"5082\",\"CITY OF PROSPECT\",2,0,0,0,0,2012,\"January\",\"Saturday\",\"03:00 pm\",\"050\",\"Pedestrian Crossing\",\"Straight road\",\"Level\",\"Not Applicable\",\"Sealed\",\"Dry\",\"Not Raining\",\"Daylight\",\"Rear End\",\"01\",\"Driver Rider\",\"1: PDO\",\"Traffic Signals\",\"\",\"\",,,']\n"
     ]
    }
   ],
   "source": [
    "# Removing the header of crash_rdd\n",
    "crash_header = crash_rdd.first()\n",
    "crash_rdd = crash_rdd.filter(lambda row: row != crash_header)   #filter out header\n",
    "print(\"Total count of crash_rdd: \", crash_rdd.count())\n",
    "print(crash_rdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not explicitly specify any partitioning strategy then by default, Spark partitions the data using Random equal partitioning unless there are specific transformations that use a different type of partitioning. So in our case as well, data is partitioned by Random equal partitioning technique. We can check the number of partitions by below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Units Data:\n",
      "Total partitions: 8\n",
      "Crash Data:\n",
      "Total partitions: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Units Data:\")\n",
    "print(f\"Total partitions: {units_rdd.getNumPartitions()}\")\n",
    "print(\"Crash Data:\")\n",
    "print(f\"Total partitions: {crash_rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do customised data partitioning according to our needs as well. For illustration I will be partitioning data into two partitions, one partition will contain all the crash data in which crashed vehicle is registered to South Australia and rest of data in other partition. We can do it easily because there is a column called Lic State which shows the state where the vehicle is registered.\n",
    "\n",
    "1st, creating a Key-Value Pair RDD with Lic State as the key and rest of the other columns as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function which will be applied to each rdd element\n",
    "def parseRecord(line):\n",
    "    lines = line.split(',')\n",
    "    array_line = []\n",
    "    for value in lines:\n",
    "        array_line.append(str(value).replace('\"', ''))\n",
    "    return (array_line[3], [x for i,x in enumerate(array_line) if i!=3] )\n",
    "units_state_rdd = units_rdd.map(parseRecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning in RDD using appropriate partitioning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to partition data into two parts, one only for SA state and other one for alll the states\n",
    "def hash_SA(key):\n",
    "    if key == 'SA':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "SA_partitioned_rdd = units_state_rdd.partitionBy(2, hash_SA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the number of records in each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF PARTITIONS: 2\n",
      "Partition 0: 218366 records\n",
      "Partition 1: 54225 records\n"
     ]
    }
   ],
   "source": [
    "#A Function to print the number of elements in each partion of the given rdd\n",
    "def print_partitions(data):\n",
    "    if isinstance(data, RDD):\n",
    "        numPartitions = data.getNumPartitions()\n",
    "        partitions = data.glom().collect()\n",
    "    else:\n",
    "        numPartitions = data.rdd.getNumPartitions()\n",
    "        partitions = data.rdd.glom().collect()\n",
    "    \n",
    "    print(f\"NUMBER OF PARTITIONS: {numPartitions}\")\n",
    "    for index, partition in enumerate(partitions):\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\")\n",
    "print_partitions(SA_partitioned_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of vechiles registered to South Australia are much higher than other parts of Australia and this is kind of abivious because we are looking into Road Crash Data of South Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some Query/Analysis. Since the article is for beginners here I will be doing some small queries. For instance, finding the average age of male and female drivers separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Male', 40.8), ('Female', 40.08)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function that will be applied to each rdd element, apply filter on it and returns only gender and age only\n",
    "def parseGenderAge(line):\n",
    "    lines = line.split(',')\n",
    "    array_line = []\n",
    "    for value in lines:\n",
    "        array_line.append(str(value).replace('\"', ''))\n",
    "    if (array_line[8] != 'XXX') & (array_line[8] != '') & (array_line[7] != 'Unknown'):\n",
    "        return (array_line[7], int(array_line[8]))\n",
    "gender_age_rdd = units_rdd.map(parseGenderAge)\n",
    "filtered_gender_age_rdd=gender_age_rdd.filter(lambda x: x is not None).filter(lambda x: x != \"\")\n",
    "gender_age_avg_rdd= filtered_gender_age_rdd.groupByKey().mapValues(lambda x: round(sum(x) / len(x),2))\n",
    "gender_age_avg_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average age of male and female involved in any crash is almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating one more query on RDDs. Display the Registration State, Year and Unit type of the vehicle of the oldest and the newest vehicle year involved in the accident?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oldest year 5 crashes:\n",
      " [(1900, ['VIC', 'Motor Cycle']), (1900, ['SA', 'Motor Cycle']), (1900, ['SA', 'Motor Cycle']), (1900, ['SA', 'Motor Cycle']), (1900, ['SA', 'Motor Cycle'])]\n",
      "\n",
      "\n",
      "Newest year 5 crashes:\n",
      " [(2019, ['SA', 'Station Wagon']), (2019, ['SA', 'OMNIBUS']), (2019, ['SA', 'Motor Cars - Sedan']), (2019, ['SA', 'Station Wagon']), (2019, ['SA', 'SEMI TRAILER'])]\n"
     ]
    }
   ],
   "source": [
    "# define a function that will be applied to each rdd element, apply filter on it and\n",
    "# returns only Registration State, Year and Unit type only\n",
    "def parseYear(line):\n",
    "    lines = line.split(',')\n",
    "    array_line = []\n",
    "    for value in lines:\n",
    "        array_line.append(str(value).replace('\"', ''))\n",
    "    if (array_line[5] != 'XXXX') & (array_line[5] != ''):\n",
    "        return int(array_line[5]), [array_line[3], array_line[4]]\n",
    "year_rdd = units_rdd.map(parseYear)\n",
    "clean_year_rdd=year_rdd.filter(lambda x: x is not None).filter(lambda x: x != \"\")\n",
    "print(\"Oldest year 5 crashes:\\n\", clean_year_rdd.sortByKey(ascending=True).collect()[0:5])\n",
    "print(\"\\n\")\n",
    "print(\"Newest year 5 crashes:\\n\", clean_year_rdd.sortByKey(ascending=False).collect()[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis using DataFrames\n",
    "Lets, discuss DataFrames 1st, A DataFrame is a distributed collection of data organized into named columns. It is equivalent to a table in a relational database or a dataframe in R/Python but with richer optimizations under the hood. For more information visit: https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "\n",
    "#### Creating DataFrames\n",
    "SparkSession provides an easy method <code>createDataFrame</code> to create Spark DataFrames. Data can be loaded from csv, json, xml and other sources like local file system or HDFS. More information on : \n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+\n",
      "| Id|     Name|Initial|\n",
      "+---+---------+-------+\n",
      "|  1|  Aaditya|      A|\n",
      "|  2|Chinnavit|      C|\n",
      "|  3|     Neha|      N|\n",
      "|  4|  Huashun|      H|\n",
      "|  5| Mohammad|      M|\n",
      "+---+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Initial: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,'Aaditya','A'),(2,'Chinnavit','C'),(3,'Neha','N'),(4,'Huashun','H'),(5,'Mohammad','M'),\n",
    "                            (10,'Prajwol', 'P'),(1,'Paras','P'),(1, 'Tooba','T'),(3, 'David','D'),(4,'Cheng','C'),(9,'Haqqani','H')],\n",
    "                           ['Id','Name','Initial'])\n",
    "#display the rows of the dataframe\n",
    "df.show(5)\n",
    "#view the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to create a DataFrame is to use the spark.read.csv file to load the data from CSV to a DataFrame or spark.read.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"bank.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Road Crash Data, loading all units and crash data into two separate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_df = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('./units_data/*.csv')\n",
    "crash_df = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('./crash_data/*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query/Analysis using PySpark DataFrame\n",
    "\n",
    "1. Finding all the crash events in Adelaide where the total number of casualties in the event is more than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(REPORT_ID='2012-4842-21/08/2019', Stats Area='1 City', Suburb='ADELAIDE', Postcode='5000', LGA Name='CITY OF ADELAIDE', Total Units='2', Total Cas=4, Total Fats='0', Total SI='0', Total MI='4', Year='2012', Month='March', Day='Thursday', Time='04:00 pm', Area Speed='050', Position Type='Not Divided', Horizontal Align='Straight road', Vertical Align='Level', Other Feat='Not Applicable', Road Surface='Sealed', Moisture Cond='Dry', Weather Cond='Not Raining', DayNight='Daylight', Crash Type='Rear End', Unit Resp='01', Entity Code='Driver Rider', CSEF Severity='2: MI', Traffic Ctrls='No Control', DUI Involved=None, Drugs Involved=None, ACCLOC_X='1327421.54', ACCLOC_Y='1669848.73', UNIQUE_LOC='13274221669849'),\n",
       " Row(REPORT_ID='2012-5769-21/08/2019', Stats Area='1 City', Suburb='ADELAIDE', Postcode='5000', LGA Name='CITY OF ADELAIDE', Total Units='2', Total Cas=4, Total Fats='0', Total SI='0', Total MI='4', Year='2012', Month='March', Day='Friday', Time='09:30 am', Area Speed='050', Position Type='Cross Road', Horizontal Align='Straight road', Vertical Align='Level', Other Feat='Not Applicable', Road Surface='Sealed', Moisture Cond='Dry', Weather Cond='Not Raining', DayNight='Daylight', Crash Type='Right Angle', Unit Resp='01', Entity Code='Driver Rider', CSEF Severity='2: MI', Traffic Ctrls='Traffic Signals', DUI Involved=None, Drugs Involved=None, ACCLOC_X='1328484.1', ACCLOC_Y='1670033.16', UNIQUE_LOC='13284841670033'),\n",
       " Row(REPORT_ID='2018-601-17/01/2020', Stats Area='1 City', Suburb='ADELAIDE', Postcode='5000', LGA Name='CITY OF ADELAIDE', Total Units='8', Total Cas=4, Total Fats='0', Total SI='2', Total MI='2', Year='2018', Month='January', Day='Sunday', Time='09:12 pm', Area Speed='050', Position Type='Not Divided', Horizontal Align='Straight road', Vertical Align='Level', Other Feat='Not Applicable', Road Surface='Sealed', Moisture Cond='Dry', Weather Cond='Not Raining', DayNight='Night', Crash Type='Hit Pedestrian', Unit Resp='01', Entity Code='Driver Rider', CSEF Severity='3: SI', Traffic Ctrls='No Control', DUI Involved=None, Drugs Involved=None, ACCLOC_X='1329806.36', ACCLOC_Y='1670224.76', UNIQUE_LOC='13298061670225'),\n",
       " Row(REPORT_ID='2017-1613-15/08/2019', Stats Area='1 City', Suburb='ADELAIDE', Postcode='5000', LGA Name='CITY OF ADELAIDE', Total Units='2', Total Cas=4, Total Fats='0', Total SI='0', Total MI='4', Year='2017', Month='February', Day='Saturday', Time='04:00 pm', Area Speed='050', Position Type='Cross Road', Horizontal Align='Straight road', Vertical Align='Level', Other Feat='Not Applicable', Road Surface='Sealed', Moisture Cond='Dry', Weather Cond='Not Raining', DayNight='Daylight', Crash Type='Right Turn', Unit Resp='01', Entity Code='Driver Rider', CSEF Severity='2: MI', Traffic Ctrls='Traffic Signals', DUI Involved=None, Drugs Involved=None, ACCLOC_X='1327951.24', ACCLOC_Y='1669556.92', UNIQUE_LOC='13279511669557'),\n",
       " Row(REPORT_ID='2017-12182-15/08/2019', Stats Area='1 City', Suburb='ADELAIDE', Postcode='5000', LGA Name='CITY OF ADELAIDE', Total Units='6', Total Cas=5, Total Fats='0', Total SI='1', Total MI='4', Year='2017', Month='December', Day='Saturday', Time='04:08 pm', Area Speed='050', Position Type='Cross Road', Horizontal Align='Straight road', Vertical Align='Level', Other Feat='Not Applicable', Road Surface='Sealed', Moisture Cond='Wet', Weather Cond='Not Raining', DayNight='Daylight', Crash Type='Hit Pedestrian', Unit Resp='01', Entity Code='Driver Rider', CSEF Severity='3: SI', Traffic Ctrls='Traffic Signals', DUI Involved=None, Drugs Involved=None, ACCLOC_X='1329016.2', ACCLOC_Y='1670995.07', UNIQUE_LOC='13290161670995'),\n",
       " Row(REPORT_ID='2019-10404-8/07/2020', Stats Area='1 City', Suburb='ADELAIDE', Postcode='5000', LGA Name='CITY OF ADELAIDE', Total Units='4', Total Cas=6, Total Fats='0', Total SI='0', Total MI='6', Year='2019', Month='October', Day='Monday', Time='08:20 am', Area Speed='060', Position Type='Divided Road', Horizontal Align='Straight road', Vertical Align='Level', Other Feat='Driveway or Entrance', Road Surface='Sealed', Moisture Cond='Dry', Weather Cond='Not Raining', DayNight='Daylight', Crash Type='Right Turn', Unit Resp='01', Entity Code='Driver Rider', CSEF Severity='2: MI', Traffic Ctrls='No Control', DUI Involved=None, Drugs Involved=None, ACCLOC_X='1327088.72', ACCLOC_Y='1670880.07', UNIQUE_LOC='13270891670880')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the type of column(\"Total Cas'\") to interger type\n",
    "crash_df = crash_df.withColumn('Total Cas',F.col('Total Cas').cast(IntegerType()))\n",
    "# applying filter\n",
    "adelaide_crash_casualty_df = crash_df.filter(col(\"Total Cas\") > 3).filter(col(\"Suburb\") == 'ADELAIDE')\n",
    "adelaide_crash_casualty_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Displaying 10 crash events with the highest casualties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "|           REPORT_ID|    Stats Area|       Suburb|Postcode|            LGA Name|Total Units|Total Cas|Total Fats|Total SI|Total MI|Year|   Month|      Day|    Time|Area Speed|Position Type|    Horizontal Align|Vertical Align|    Other Feat|Road Surface|Moisture Cond|Weather Cond|DayNight| Crash Type|Unit Resp| Entity Code|CSEF Severity|  Traffic Ctrls|DUI Involved|Drugs Involved|  ACCLOC_X|  ACCLOC_Y|    UNIQUE_LOC|\n",
      "+--------------------+--------------+-------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "| 2017-288-15/08/2019|2 Metropolitan|   PARA HILLS|    5096|   CITY OF SALISBURY|          2|       11|         0|       1|      10|2017| January|Wednesday|01:13 pm|       060|   T-Junction|       Straight road| Crest of Hill|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|Right Angle|       01|Driver Rider|        3: SI|      Stop Sign|        null|          null| 1334428.9|1683032.96|13344291683033|\n",
      "|2016-6630-15/08/2019|2 Metropolitan|KANGAROO FLAT|    5118|LIGHT REGIONAL CO...|          3|        9|         0|       2|       7|2016|   April|Wednesday|09:00 pm|       100|  Not Divided|CURVED, VIEW OBSC...|         Level|Not Applicable|      Sealed|          Dry| Not Raining|   Night|    Head On|       01|Driver Rider|        3: SI|     No Control|        null|          null|1339316.32|1710314.92|13393161710315|\n",
      "|2014-14065-21/08/...|2 Metropolitan|MODBURY NORTH|    5092|CITY OF TEA TREE ...|          5|        9|         0|       0|       9|2014|November|  Tuesday|03:09 pm|       040| Divided Road|       Straight road|         Level|     Roadworks|      Sealed|          Dry| Not Raining|Daylight|   Rear End|       01|Driver Rider|        2: MI|     No Control|        null|          null|1335990.18|1682195.98|13359901682196|\n",
      "|2014-9004-21/08/2019|     3 Country|          APY|     872|                null|          2|        9|         1|       0|       8|2014|    July|   Friday|07:20 pm|       100|  Not Divided|       Straight road| Crest of Hill|Not Applicable|    Unsealed|          Dry| Not Raining|   Night|  Roll Over|       01|Driver Rider|     4: Fatal|     No Control|        null|          null| 706774.16| 2586947.5|07067742586948|\n",
      "|2019-11734-8/07/2020|2 Metropolitan|        STURT|    5047|CC MARION.       ...|          2|        9|         0|       1|       8|2019|November|   Sunday|07:25 pm|       060|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight| Right Turn|       02|Driver Rider|        3: SI|Traffic Signals|        null|          null|1324428.84|1659884.95|13244291659885|\n",
      "|2016-3035-15/08/2019|2 Metropolitan|      HACKHAM|    5163| CITY OF ONKAPARINGA|          3|        9|         3|       5|       1|2016| January| Saturday|11:50 am|       080|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight| Right Turn|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1320361.49|1645195.63|13203611645196|\n",
      "|2014-8966-21/08/2019|     3 Country|     WINTINNA|    5723|                null|          1|        8|         2|       3|       3|2014|    July|   Friday|10:50 am|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|  Roll Over|       01|Driver Rider|     4: Fatal|     No Control|        null|          null| 906446.43|2454222.49|09064462454222|\n",
      "|2015-2823-21/08/2019|     3 Country|       HAWKER|    5434|THE FLINDERS RANG...|          1|        8|         0|       0|       8|2015|   March|   Monday|06:00 pm|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|  Roll Over|       01|Driver Rider|        2: MI|     No Control|        null|          null|1315077.61|2022309.34|13150782022309|\n",
      "|2016-14407-15/08/...|     3 Country|    STOCKWELL|    5355|THE BAROSSA COUNCIL.|          2|        8|         1|       6|       1|2016| October|   Sunday|03:46 pm|       100|  Not Divided|       Straight road| Crest of Hill|Not Applicable|    Unsealed|          Dry| Not Raining|Daylight|    Head On|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1373964.45|1723462.57|13739641723463|\n",
      "|2016-7073-15/08/2019|     3 Country|     MERRITON|    5523|PT.PIRIE CITY & D...|          2|        8|         4|       3|       1|2016|   April|   Sunday|12:35 pm|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Head On|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1293759.89|1840109.96|12937601840110|\n",
      "+--------------------+--------------+-------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crash_df.sort(col('Total Cas'), ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Finding the total number of fatalities for each crash type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|          Crash Type|Total fatalities|\n",
      "+--------------------+----------------+\n",
      "|    Hit Fixed Object|           245.0|\n",
      "|             Head On|           136.0|\n",
      "|      Hit Pedestrian|           109.0|\n",
      "|           Roll Over|            93.0|\n",
      "|         Right Angle|            82.0|\n",
      "|            Rear End|            33.0|\n",
      "|          Side Swipe|            30.0|\n",
      "|          Right Turn|            26.0|\n",
      "|  Hit Parked Vehicle|            10.0|\n",
      "|          Hit Animal|             7.0|\n",
      "|               Other|             5.0|\n",
      "|Left Road - Out o...|             3.0|\n",
      "|  Hit Object on Road|             2.0|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crash_df.groupBy('Crash Type').agg(F.sum('Total Fats').alias('Total fatalities')).sort(col('Total fatalities'), ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Displaying the name of the suburb and the total number of casualties for each suburb when the vehicle was driven by an unlicensed driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|         Suburb|Total casualty|\n",
      "+---------------+--------------+\n",
      "|       ADELAIDE|            43|\n",
      "|      SALISBURY|            30|\n",
      "|       PROSPECT|            27|\n",
      "|     INGLE FARM|            22|\n",
      "| SALISBURY EAST|            20|\n",
      "|  MORPHETT VALE|            20|\n",
      "|      DRY CREEK|            19|\n",
      "|  MURRAY BRIDGE|            19|\n",
      "| NORTH ADELAIDE|            17|\n",
      "|   MAWSON LAKES|            17|\n",
      "|        ENFIELD|            16|\n",
      "|SALISBURY DOWNS|            15|\n",
      "|    GEPPS CROSS|            15|\n",
      "|   REGENCY PARK|            15|\n",
      "| ELIZABETH PARK|            14|\n",
      "|PARA HILLS WEST|            14|\n",
      "|         SEATON|            14|\n",
      "|   BEDFORD PARK|            13|\n",
      "|   DAVOREN PARK|            13|\n",
      "|  MOUNT GAMBIER|            13|\n",
      "+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# joining both dataframes\n",
    "complete_df = crash_df.join(units_df,crash_df.REPORT_ID==units_df.REPORT_ID,how='inner')\n",
    "by_suburb_no_lic_no_df = complete_df.filter(col('Licence Type') == 'Unlicenced').groupby('Suburb').agg(F.sum('Total Cas').alias('Total casualty'))\n",
    "by_suburb_no_lic_no_df.sort(col('Total casualty'), ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data set, the severity of the crash is given by the column “CSEF Severity”, the three levels of severity is given in the Metadata file. Similarly, the columns “DUI Involved” and “Drugs Involved” tell whether the driver has been detected with blood alcohol and drugs respectively.\n",
    "\n",
    "With this information given in data set, we can analyze whether the severity of accidents is higher when the driver is on drugs or alcohol compared to when the driver is normal.\n",
    "\n",
    "The total number of crash events for each severity level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|CSEF Severity|Total accidents|\n",
      "+-------------+---------------+\n",
      "|     4: Fatal|            722|\n",
      "|        2: MI|          37300|\n",
      "|       1: PDO|          84775|\n",
      "|        3: SI|           4875|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "severe_df = crash_df.groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('Total accidents'))\n",
    "severe_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of crash events for each severity level and the percentage for the four different scenarios.\n",
    "\n",
    "a) When the driver is tested positive on drugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------+\n",
      "|CSEF Severity|Total accidents|percent_share|\n",
      "+-------------+---------------+-------------+\n",
      "|     4: Fatal|            117|        6.31%|\n",
      "|        2: MI|           1109|       59.82%|\n",
      "|       1: PDO|            237|       12.78%|\n",
      "|        3: SI|            391|       21.09%|\n",
      "+-------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "severe_on_drugs_df = crash_df.filter(col('Drugs Involved') == 'Y').groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('Total accidents'))\n",
    "total_cas_on_drugs = severe_on_drugs_df.groupBy().sum().collect()[0][0]\n",
    "def find_severe_percent(s):\n",
    "    return str((round((int(s)/total_cas_on_drugs)*100, 2)))+\"%\"\n",
    "find_severe_percent_udf = udf(find_severe_percent, StringType())\n",
    "severe_on_drugs_percent_df = severe_on_drugs_df.withColumn('percent_share',find_severe_percent_udf('Total accidents'))\n",
    "severe_on_drugs_percent_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) When the driver is tested positive for blood alcohol concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------+\n",
      "|CSEF Severity|Total accidents|percent_share|\n",
      "+-------------+---------------+-------------+\n",
      "|     4: Fatal|            136|        3.42%|\n",
      "|        2: MI|           1243|       31.24%|\n",
      "|       1: PDO|           2144|       53.88%|\n",
      "|        3: SI|            456|       11.46%|\n",
      "+-------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "severe_on_alcohol_df = crash_df.filter(col('DUI Involved') == 'Y').groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('Total accidents'))\n",
    "total_cas_on_alcohol = severe_on_alcohol_df.groupBy().sum().collect()[0][0]\n",
    "def find_severe_percent(s):\n",
    "    return str((round((int(s)/total_cas_on_alcohol)*100, 2)))+\"%\"\n",
    "find_severe_percent_udf = udf(find_severe_percent, StringType())\n",
    "severe_on_alcohol_percent_df = severe_on_alcohol_df.withColumn('percent_share',find_severe_percent_udf('Total accidents'))\n",
    "severe_on_alcohol_percent_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) When the driver is tested positive for both drugs and blood alcohol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------+\n",
      "|CSEF Severity|Total accidents|percent_share|\n",
      "+-------------+---------------+-------------+\n",
      "|     4: Fatal|             39|       13.09%|\n",
      "|        2: MI|            151|       50.67%|\n",
      "|       1: PDO|             36|       12.08%|\n",
      "|        3: SI|             72|       24.16%|\n",
      "+-------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "severe_on_alcohol_drugs_df = crash_df.filter((col('DUI Involved') == 'Y') & (col('Drugs Involved') == 'Y')).groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('Total accidents'))\n",
    "total_cas_on_alcohol_drugs = severe_on_alcohol_drugs_df.groupBy().sum().collect()[0][0]\n",
    "def find_severe_percent(s):\n",
    "    return str((round((int(s)/total_cas_on_alcohol_drugs)*100, 2)))+\"%\"\n",
    "    \n",
    "find_severe_percent_udf = udf(find_severe_percent, StringType())\n",
    "severe_on_alcohol_drugs_percent_df = severe_on_alcohol_drugs_df.withColumn('percent_share',find_severe_percent_udf('Total accidents'))\n",
    "severe_on_alcohol_drugs_percent_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) When the driver is tested negative for both (no alcohol and no drugs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------+\n",
      "|CSEF Severity|Total accidents|percent_share|\n",
      "+-------------+---------------+-------------+\n",
      "|     4: Fatal|            508|        0.42%|\n",
      "|        2: MI|          35099|       28.74%|\n",
      "|       1: PDO|          82430|       67.49%|\n",
      "|        3: SI|           4100|        3.36%|\n",
      "+-------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "without_drugs_alcohol_df = crash_df.filter((col('DUI Involved').isNull()) & (col('Drugs Involved').isNull()))\n",
    "severe_without_drugs_alcohol_df = without_drugs_alcohol_df.groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('Total accidents'))\n",
    "total_cas_without_alcohol_drugs = severe_without_drugs_alcohol_df.groupBy().sum().collect()[0][0]\n",
    "def find_severe_percent(s):\n",
    "    return str((round((int(s)/total_cas_without_alcohol_drugs)*100, 2)))+\"%\"\n",
    "    \n",
    "find_severe_percent_udf = udf(find_severe_percent, StringType())\n",
    "severe_without_alcohol_drugs_percent_df = severe_without_drugs_alcohol_df.withColumn('percent_share',find_severe_percent_udf('Total accidents'))\n",
    "severe_without_alcohol_drugs_percent_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the results from the above four different scenarios to a single table and visualize it with a bar graph and hence We can compare and analyse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_positive_df = crash_df.filter(col('Drugs Involved') == 'Y').groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('On Drugs'))\n",
    "alcohol_positive_df = crash_df.filter(col('DUI Involved') == 'Y').groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('On Alcohol'))\n",
    "alcohol_drugs_positive_df = crash_df.filter((col('DUI Involved') == 'Y') & (col('Drugs Involved') == 'Y')).groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('On Both'))\n",
    "without_drugs_alcohol_df = crash_df.filter((col('DUI Involved').isNull()) & (col('Drugs Involved').isNull()))\n",
    "drugs_alcohol_negative_df = without_drugs_alcohol_df.groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('On None'))\n",
    "all_df = drugs_positive_df.join(alcohol_positive_df, [\"CSEF Severity\"]).join(alcohol_drugs_positive_df, [\"CSEF Severity\"]).join(drugs_alcohol_negative_df, [\"CSEF Severity\"])\n",
    "# all_df.show()\n",
    "total_cas_on_drugs = drugs_positive_df.groupBy().sum().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+-------+-------+\n",
      "|CSEF Severity|On Drugs|On Alcohol|On Both|On None|\n",
      "+-------------+--------+----------+-------+-------+\n",
      "|     4: Fatal|     117|       136|     39|    508|\n",
      "|        2: MI|    1109|      1243|    151|  35099|\n",
      "|       1: PDO|     237|      2144|     36|  82430|\n",
      "|        3: SI|     391|       456|     72|   4100|\n",
      "+-------------+--------+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_severe_percent1(s):\n",
    "    return float(round((int(s)/total_cas_on_drugs)*100,2))\n",
    "    \n",
    "find_severe_percent_udf1 = udf(find_severe_percent1, FloatType())\n",
    "drugs_positive_df_per = drugs_positive_df.withColumn('On Drugs',find_severe_percent_udf1('On Drugs'))\n",
    "total_cas_on_alcohol = alcohol_positive_df.groupBy().sum().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_severe_percent2(s):\n",
    "    return float(round((int(s)/total_cas_on_alcohol)*100,2))\n",
    "find_severe_percent_udf2 = udf(find_severe_percent2, FloatType())\n",
    "alcohol_positive_df_per = alcohol_positive_df.withColumn('On Alcohol',find_severe_percent_udf2('On Alcohol'))\n",
    "total_cas_on_drugs = alcohol_drugs_positive_df.groupBy().sum().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_severe_percent3(s):\n",
    "    return float(round((int(s)/total_cas_on_drugs)*100,2))\n",
    "    \n",
    "find_severe_percent_udf3 = udf(find_severe_percent3, FloatType())\n",
    "alcohol_drugs_positive_df_per = alcohol_drugs_positive_df.withColumn('On Both',find_severe_percent_udf3('On Both'))\n",
    "\n",
    "total_cas_on_alcohol = drugs_alcohol_negative_df.groupBy().sum().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_severe_percent4(s):\n",
    "    return float(round((int(s)/total_cas_on_alcohol)*100,2))\n",
    "find_severe_percent_udf4 = udf(find_severe_percent4, FloatType())\n",
    "drugs_alcohol_negative_df_per = drugs_alcohol_negative_df.withColumn('On None',find_severe_percent_udf4('On None'))\n",
    "all_df_per = drugs_positive_df_per.join(alcohol_positive_df_per, [\"CSEF Severity\"]).join(alcohol_drugs_positive_df_per, [\"CSEF Severity\"]).join(drugs_alcohol_negative_df_per, [\"CSEF Severity\"])\n",
    "# all_df_per.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = all_df.toPandas()\n",
    "pd_df_per = all_df_per.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you run the above code your system might crash. Because when we ware using PySpark DataFrame, PySpark is managing the dataframe using parallel computing and equal data distribution management etc and in above code, we are converting PySpark Dataframe to python Dataframe and if the size of dataframe is huge then your system will run out of memory because python dataframe is not distributed. This the power of PySpark, PySpark can handle Bug Data whereas if you do it normally then it will require a huge amount of computation power and memory which will not be feasible.\n",
    "\n",
    "If converting PySpark dataframe to python dataframe is such a problem then why we are converting it? The answer is, in PySpark we do not have any library which can do plotting using parallel computing so we have to convert our PySpark dataframe into Python dataframe so that we can use matplot lib for plotting and one more thing to note is that matplot lib also does not support parallel computing. In this situation as well if data is huge and your machine can crash while plotting because plotting use huge amount of memory and since matplot lib is not able to do parallel computing your system might crash.\n",
    "\n",
    "Hope your system will have enough memory otherwise try to do resampling of data to reduce the data size and then do all the above steps and below step of plotting the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.plot(kind='bar', x = 'CSEF Severity', )\n",
    "plt.savefig('severe.png')\n",
    "pd_df_per.plot(kind='bar', x = 'CSEF Severity', )\n",
    "plt.savefig('severe_per.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are confused with all the operations on PySpark DataFrames or SparkSQL then you can download my tutorial on PySpark DataFrames and SparkSQL from my [github repository](https://github.com/RakeshNain/Big-data-Analysis-of-Road-Crash-Data-using-PySpark-with-PySpark-Tutorial.git), jupyter notebook name is \"Tutorial on PySpark DataFrames and SparkSQL\" and data files required for \"Tutorial on PySpark DataFrames and SparkSQL\" are in the folder \"data for given tutorials\".\n",
    "\n",
    "In PySpark we can implement the same queries using RDDs, DataFrame and SparkSQL. I am going to show you a few queries implemented with three different methods RDDs, DataFrame and SparkSQL.\n",
    "\n",
    "1. Finding the Date and Time of Crash, Number of Casualties in each unit and the Gender, Age, License Type of the unit driver for the suburb “Adelaide”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Implementation\n",
    "def parseKey(line):\n",
    "    lines = line.split(',')\n",
    "    array_line = []\n",
    "    for value in lines:\n",
    "        array_line.append(str(value).replace('\"', ''))\n",
    "    return (array_line[0], array_line[1:] )\n",
    "units_rdd1 = units_rdd.map(parseKey)\n",
    "crash_rdd1 = crash_rdd.map(parseKey)\n",
    "joined_rdd = units_rdd1.join(crash_rdd1)\n",
    "def parseRecord1(line):\n",
    "    \n",
    "    if line[1][1][1] == 'ADELAIDE':\n",
    "        array_line = []\n",
    "        array_line.append(line[1][1][1])\n",
    "        array_line.append(line[1][0][7])\n",
    "        array_line.append(line[1][0][8])\n",
    "        array_line.append(line[1][0][11])\n",
    "        array_line.append(line[1][1][12])\n",
    "        array_line.append(line[1][1][6])\n",
    "        array_line.append(line[1][1][9] + \"-\" + line[1][1][10] + \"-\" + line[1][1][11])\n",
    "        return (array_line)\n",
    "    \n",
    "pharsed_rdd = joined_rdd.map(parseRecord1)\n",
    "filtered_pharsed_rdd=pharsed_rdd.filter(lambda x: x is not None)\n",
    "filtered_pharsed_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame implementation\n",
    "joined_df = crash_df.join(units_df,crash_df.REPORT_ID==units_df.REPORT_ID, how='inner')\n",
    "def find_date(d,m,y):\n",
    "    return y + \"-\" + m + \"-\" + d\n",
    "    \n",
    "find_date_udf = udf(find_date)\n",
    "joined_date_df = joined_df.withColumn('Date',find_date_udf('Day', 'Month', 'Year'))\n",
    "joined_date_df = joined_date_df.filter(col(\"Suburb\") == 'ADELAIDE')\n",
    "joined_date_df.select('Date', 'Time', 'Total Cas', 'Sex', 'Age', 'Licence Type', 'Suburb').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSQL implementation\n",
    "units_df.createOrReplaceTempView(\"units_table\")\n",
    "crash_df.createOrReplaceTempView(\"crash_table\")\n",
    "joined_table = spark.sql('''\n",
    "  SELECT (Year || '-' || Month || '-' || Day) as Date, Time, `Total Cas`, Sex, Age, `Licence Type`, Suburb\n",
    "  FROM units_table u JOIN crash_table c\n",
    "  ON u.REPORT_ID = c.REPORT_ID\n",
    "  where c.Suburb == 'ADELAIDE'\n",
    "''')\n",
    "joined_table.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Finding the total number of casualties for each suburb when the vehicle was driven by an unlicensed driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD implementation\n",
    "def parseKey(line):\n",
    "    lines = line.split(',')\n",
    "    array_line = []\n",
    "    for value in lines:\n",
    "        array_line.append(str(value).replace('\"', ''))\n",
    "    return (array_line[0], array_line[1:] )\n",
    "units_rdd1 = units_rdd.map(parseKey)\n",
    "crash_rdd1 = crash_rdd.map(parseKey)\n",
    "joined_rdd = units_rdd1.join(crash_rdd1)\n",
    "def parseRecord2(line):\n",
    "    return (line[1][1][1], int(line[1][1][5]))\n",
    "    \n",
    "filtered_rdd = joined_rdd.filter(lambda x: (x[1][0][9] != \"XX\") & (x[1][0][8] != \"UNKNOWN\") & (x[1][0][10] != \"Unknown\") & (x[1][0][9] is not None) & (x[1][0][10] is not None) & (x[1][0][11] is not None))\n",
    "pharased_filtered_rdd = filtered_rdd.map(parseRecord2)\n",
    "result= pharased_filtered_rdd.groupByKey().mapValues(lambda x: sum(x))\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame implementation\n",
    "joined_df = crash_df.join(units_df,crash_df.REPORT_ID==units_df.REPORT_ID, how='inner')\n",
    "without_lic_df = joined_df.dropna(subset=('Licence Type','Licence Class', 'Lic State'), how='all')\n",
    "by_suburb_no_lic_no_df = without_lic_df.filter((col('Licence Type') != 'Unknown') &(col('Licence Class') != 'XX') &(col('Lic State') != 'UNKNOWN')).groupby('Suburb').agg(F.count('No Of Cas').alias('Total casualty'))\n",
    "by_suburb_no_lic_no_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSQL implementation\n",
    "units_df.createOrReplaceTempView(\"units_table\")\n",
    "crash_df.createOrReplaceTempView(\"crash_table\")\n",
    "joined_table = spark.sql('''\n",
    "  SELECT Suburb, COUNT(`Total Cas`)\n",
    "  FROM units_table u JOIN crash_table c\n",
    "  ON u.REPORT_ID = c.REPORT_ID\n",
    "  WHERE `Lic State` IS NOT NULL and `Licence Class` IS NOT NULL and `Licence Type` IS NOT NULL and\n",
    "  `Licence Type` != 'Unknown' and\n",
    "  `Licence Class` != 'XX' and\n",
    "  `Lic State` != 'UNKNOWN'\n",
    "GROUP BY Suburb\n",
    "''')\n",
    "joined_table.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some visualisations to make little better analysis. But in PySpark we do not have any library which can do plotting using parallel computing so we have converted out PySpark dataframe into Python dataframe so that we can use matplot lib for plotting and matplot lib does not support parallel computing. In this situation, if data is huge then you machine can crash while plotting so you can use techniques like resampling.\n",
    "\n",
    "So let's plot the total number of crash events for each severity level and the percentage for the four different scenarios all together in single bar graph where four scenarios are:\n",
    "\n",
    "a) When the driver is tested positive on drugs. \n",
    "\n",
    "b) When the driver is tested positive for blood alcohol concentration. \n",
    "\n",
    "c) When the driver is tested positive for both drugs and blood alcohol \n",
    "\n",
    "d) When the driver is tested negative for both (no alcohol and no drugs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_positive_df = crash_df.filter(col('Drugs Involved') == 'Y').groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('On Drugs'))\n",
    "alcohol_positive_df = crash_df.filter(col('DUI Involved') == 'Y').groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('On Alcohol'))\n",
    "alcohol_drugs_positive_df = crash_df.filter((col('DUI Involved') == 'Y') & (col('Drugs Involved') == 'Y')).groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('On Both'))\n",
    "without_drugs_alcohol_df = crash_df.filter((col('DUI Involved').isNull()) & (col('Drugs Involved').isNull()))\n",
    "drugs_alcohol_negative_df = without_drugs_alcohol_df.groupby('CSEF Severity').agg(F.count(crash_df.REPORT_ID).alias('On None'))\n",
    "all_df = drugs_positive_df.join(alcohol_positive_df, [\"CSEF Severity\"]).join(alcohol_drugs_positive_df, [\"CSEF Severity\"]).join(drugs_alcohol_negative_df, [\"CSEF Severity\"])\n",
    "all_df.show()\n",
    "total_cas_on_drugs = drugs_positive_df.groupBy().sum().collect()[0][0]\n",
    "def find_severe_percent1(s):\n",
    "    return float(round((int(s)/total_cas_on_drugs)*100,2))\n",
    "    \n",
    "find_severe_percent_udf1 = udf(find_severe_percent1, FloatType())\n",
    "drugs_positive_df_per = drugs_positive_df.withColumn('On Drugs',find_severe_percent_udf1('On Drugs'))\n",
    "total_cas_on_alcohol = alcohol_positive_df.groupBy().sum().collect()[0][0]\n",
    "def find_severe_percent2(s):\n",
    "    return float(round((int(s)/total_cas_on_alcohol)*100,2))\n",
    "find_severe_percent_udf2 = udf(find_severe_percent2, FloatType())\n",
    "alcohol_positive_df_per = alcohol_positive_df.withColumn('On Alcohol',find_severe_percent_udf2('On Alcohol'))\n",
    "total_cas_on_drugs = alcohol_drugs_positive_df.groupBy().sum().collect()[0][0]\n",
    "def find_severe_percent3(s):\n",
    "    return float(round((int(s)/total_cas_on_drugs)*100,2))\n",
    "    \n",
    "find_severe_percent_udf3 = udf(find_severe_percent3, FloatType())\n",
    "alcohol_drugs_positive_df_per = alcohol_drugs_positive_df.withColumn('On Both',find_severe_percent_udf3('On Both'))\n",
    "\n",
    "\n",
    "total_cas_on_alcohol = drugs_alcohol_negative_df.groupBy().sum().collect()[0][0]\n",
    "def find_severe_percent4(s):\n",
    "    return float(round((int(s)/total_cas_on_alcohol)*100,2))\n",
    "find_severe_percent_udf4 = udf(find_severe_percent4, FloatType())\n",
    "drugs_alcohol_negative_df_per = drugs_alcohol_negative_df.withColumn('On None',find_severe_percent_udf4('On None'))\n",
    "all_df_per = drugs_positive_df_per.join(alcohol_positive_df_per, [\"CSEF Severity\"]).join(alcohol_drugs_positive_df_per, [\"CSEF Severity\"]).join(drugs_alcohol_negative_df_per, [\"CSEF Severity\"])\n",
    "all_df_per.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you run the above code your system might crash. Because when we were using PySpark DataFrame, PySpark is managing the dataframe using parallel computing and equal data distribution management etc and in above code, we are converting PySpark Dataframe to python Dataframe and if the size of dataframe is huge then your system will run out of memory because python dataframe is not distributed. This the power of PySpark, PySpark can handle Big Data whereas if you do it normally then it will require a huge amount of computation power and memory which will not be feasible.\n",
    "\n",
    "If converting PySpark dataframe to python dataframe is such a problem then why we are converting it? The answer is, in PySpark we do not have any library which can do plotting using parallel computing so we have to convert our PySpark dataframe into Python dataframe so that we can use matplot lib for plotting and one more thing to note is that matplot lib also does not support parallel computing. In this situation as well if data is huge and your machine can crash while plotting because plotting use huge amount of memory and since matplot lib is not able to do parallel computing, your system might crash.\n",
    "\n",
    "Hope your system will have enough memory otherwise try to do resampling of data to reduce the data size and then do all the above steps and below step of plotting the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df_per = all_df_per.toPandas()\n",
    "pd_df_per.plot(kind='bar', x = 'CSEF Severity', )\n",
    "plt.savefig('severe_per.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark we can implement the same queries using RDDs, DataFrame and SparkSQL. I am going to show you a few queries implemented with three different methods RDDs, DataFrame and SparkSQL.\n",
    "\n",
    "1. Finding the Date and Time of Crash, Number of Casualties in each unit and the Gender, Age, License Type of the unit driver for the suburb “Adelaide”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
